\documentclass{article}
\usepackage[]{algorithm2e}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{cleveref}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

\input{eqn_abbr}

\title{Reversible jump MCMC}
\date{}
\begin{document}
\maketitle




Reversible jump MCMC is a Bayesian algorithm to infer the number of components/ clusters from a set of data. For this illustration we shall consider a two component model at most.

The difficulty lies in considering the posterior given different number of components.

\section{Model}
The likelihoods can be represented as:
\begin{align}
p(y_i|\lambda_{d\cdot},k=state,z_{di})=&\prod_{j=1}^{d} (\lambda_{dj}\exp(-\lambda_{dj}y_i))^{1(z_{di}=j)}
\end{align}
where $d$ is the current dimensionality of the model. The priors on the latent variables are:
\begin{align}
p(\lambda_{dj})\propto & \frac{1}{\lambda_{dj}}\qquad \lambda_{dj}\in[a,b]\\
p(z_{di}=j)=&\pi_{dj}\\
p(\pi_{dj}) = & \text{Dir}(\alpha)\\
p(d=k)= & 1/K
\end{align}
$z_{di}$ is an indicator variable which chooses the relevant $\lambda_{dj}$ for the $i$-th observation. $K$ is the number of models that are being considered with differing dimensionality. $p(d=k)= 1/K$ implies that each model is equally probable. The limits on $\lambda_{dj}$ are such that $a=\min(1/y_i)$ and $b=\max(1/y_i)$ for all $j,k$. This is due to the fact that the mean of an exponential distribution is $1/\lambda$. Hence, it is a valid assumption that the mean of $1/y_i$ lies between $a$ and $b$. $\alpha$ is chosen to be $1$ such that it portrays a uniform distribution over $[0,1]$.

Marginalising out $z_{dj}$ we obtain:
\begin{align}
p(y_i|\lambda_{d\cdot},d=k)= \sum_{j=1}^{k}\pi_{kj}\lambda_{kj}\exp(-\lambda_{kj}y_i)
\label{eq:lik}
\end{align}

\section{Jumping dimensions}
We need to consider a Metropolis-Hastings (MH) step to consider going from one component to two components. The MH step in general is as follows:
\begin{align}
\alpha = & \frac{p\clrbracket{\theta_{d(t+1)}|\cy}}{p\clrbracket{\theta_{d(t)}|\cy}}\frac{q\clrbracket{\theta_{d(t)}|\theta_{d(t+1)}}}{q\clrbracket{\theta_{d(t+1)}|\theta_{d(t)}}}\label{eq:alpha}\\
A = & \text{min}\left(1,\alpha\right)
\end{align}
where $d(t)$ is the dimensionality at time $t$ and $\theta_{d(t)}=\{\lambda_{d(t)\cdot},\pi_{d(t)\cdot}\}$. The proportion $\frac{p\clrbracket{\theta_{d(t+1)}|\cy}}{p\clrbracket{\theta_{d(t)}|\cy}}$ can be rewritten as,
\begin{align}
\frac{p\clrbracket{\theta_{d(t+1)},\cy}}{p\clrbracket{\theta_{d(t)},\cy}}
\end{align}
since $p(\cy)$ is independent of dimensionality and, can be evaluated using equation \ref{eq:lik} as,
\begin{align}
\frac{\clrbracket{\prod_{i=1}^{N}\sum_{j=1}^{d(t+1)}\pi_{kj}\lambda_{kj}\exp(-\lambda_{kj}y_i)}\prod_{j=1}^{d(t+1)}p(\lambda_{d(t+1)j})p(\pi_{d(t+1)})p(d=d(t+1))}{\clrbracket{\prod_{i=1}^{N}\sum_{j=1}^{d(t)}\pi_{kj}\lambda_{kj}\exp(-\lambda_{kj}y_i)}\prod_{j=1}^{d(t)}p(\lambda_{d(t)j})p(\pi_{d(t)})p(d=d(t))}
\label{eq:lik_ratio}
\end{align}
The proposal distributions are as follows:
%(for $d(t)=d(t+1)-1$)
\begin{align}
q\clrbracket{\theta_{d(t+1)}|\theta_{d(t)}}=q(d(t+1)|d(t))q(idx)q(\lambda_{d(t+1)}|\lambda_{d(t)},idx)q(\pi_{d(t+1)}|\pi_{d(t)},idx)
\end{align}

We propose the dimensionality jump as follows:
\begin{align}
q(d(t+1)=d(t)-1|d(t))=1\qquad&\text{if }d(t)=K\\
q(d(t+1)=d(t)+1|d(t))=1\qquad&\text{if }d(t)=1\\
q(d(t+1)=d(t)-1|d(t))=q(d(t+1)=d(t)+1|d(t))=0.5\qquad&\text{otherwise}
\end{align}
where $K$ is the highest dimensionality encountered in this model.

With probability
\begin{align}
q(idx)=\frac{1}{\min(d(t),d(t+1))}
\end{align}
an index is chosen to split if jumping to a higher dimension or merge if jumping to a lower dimension. If the current dimensionality is $k$ and it is required to jump to a lower dimension an element from the set of tuples, $\{1,2\},\{2,3\},\cdots,\{k-1,k\}$ is chosen. Thus the probability of choosing a certain index is the same regardless of jumping to a higher or lower dimension.

\subsection{Jumping from dimension $k$ to $k+1$}
In order to evaluate $q(\lambda_{[k+1]\cdot}|\lambda_{[k]\cdot}, idx)$ we use the following proposal,
\begin{align}
\mu_1\sim & \,U(0,1)\\
\lambda_{[k+1]idx}=&\lambda_{[k]idx}\frac{\mu_1}{1-\mu_1}
\label{eq:lambda_12}\\
\lambda_{[k+1]idx+1}=&\lambda_{[k]idx}\frac{1-\mu_1}{\mu_1}
\label{eq:lambda_22}\\
\lambda_{[k+1]/idx}=&\lambda_{[k]/idx}
\end{align} 
%\pi=&\mu_2
where $/idx$ are all indices but the index chosen at random.

Thus, in order to find the distribution $q(\lambda_{[k+1],idx},\lambda_{[k+1],idx+1}\vert \lambda_{[k],idx})$ we use the change of variable identity that $q(\lambda_{[k+1],idx},\lambda_{[k+1],idx+1}\vert \lambda_{[k],idx})=q(\mu_1)\vert J\vert$ where, $J$ is the Jacobian $\frac{\partial(\lambda_{[k],idx},\mu_1)}{\partial(\lambda_{[k+1]idx},\lambda_{[k+1]idx+1})}$. Using \crefrange{eq:lambda_12}{eq:lambda_22} the inverse Jacobian is found to be:
\begin{align}
J^{-1}=\begin{bmatrix}
\frac{\mu_1}{1-\mu_1} & \lambda_{[k],idx}\frac{1}{(1-\mu_1)^2}\\
\frac{1-\mu_1}{\mu_1} & -\lambda_{[k],idx}\frac{1}{\mu_1^2}
\end{bmatrix}
\end{align}
Hence the determinant is of the Jacobian determinant is found to be $\frac{\mu_1(1-\mu_1)}{2\lambda_{[k],idx}}$ while $q(\mu_1)=1$ since they are sampled from standard uniform distributions. We also propose that $q(\lambda_{[k+1]/idx}=\lambda_{[k]/idx})=1$.

In order to evaluate $q(\pi_{[k+1]\cdot}|\pi_{[k]\cdot}, idx)$ we use the following proposal,
\begin{align}
\mu_2\sim & \,U(0,1)\\
\pi_{[k+1]idx}=&\pi_{[k]idx}\mu_2 \label{eq:pi_12}\\
\pi_{[k+1]idx+1}=&\pi_{[k]idx}(1-\mu_2) \label{eq:pi_22}\\
\pi_{[k+1]/idx}=&\pi_{[k]/idx}
\end{align}
Thus, $q(\pi_{[k+1]idx},\pi_{[k+1]idx+1}|\pi_{[k]idx})=q(\mu_2)\frac{1}{\pi_{[k]idx}}$ using the change of variable formula as before. Similarly, we let $q(\pi_{[k+1]/idx}=\pi_{[k]/idx})=1$.

\subsection{Jumping from $k+1$ to $k$}
After an index $idx$ is chosen we combine dimensions with the following proposals:
\begin{align}
q(\lambda_{[k],idx}=\sqrt{\lambda_{[k+1],idx}\lambda_{[k+1],idx+1}})=1\\
q(\pi_{[k]idx}=\pi_{[k+1]idx}+\pi_{[k+1]idx+1})=1
\end{align}
These equations can be derived from \crefrange{eq:lambda_12}{eq:lambda_22} and \crefrange{eq:pi_12}{eq:pi_22}. Also note that, $\mu_1=\frac{\lambda_{12}}{\lambda_{[k],idx}+\lambda_{12}}$ which can be derived from equation \ref{eq:lambda_12}.

%Since we need the ratio of proposed states $ \frac{q(\theta_1^t|\theta_2^{t+1})}{q(\theta_2^{t+1}|\theta_1^{t})} $ we are also required to find $ q(\lambda_{[k],idx},k=1\vert\lambda_{2j},\pi,k=2) = q(\lambda_{[k],idx}\vert\lambda_{2j},k=1) q(k=1 \vert k=2) $. We again take $ q(k=1\vert k=2)=1 $. $q(\lambda_{[k],idx}=\sqrt{\lambda_{12}\lambda_{22}})=1$, i.e. support at only point and zero at all other values of $\lambda_{[k],idx}$.
%
%
%The MH step is conducted using the reciprocal of $\alpha$ in the equation above. Apart from the identity that $\lambda_{[k],idx}=\sqrt{\lambda_{12}\lambda_{22}}$ we also require the equation,  $\mu_2$ which is equivalent to $\pi$ can be taken to be the previous $\pi=\mu_2$ that was generated at state 2.

\subsection{MH ratio}
The proportion $\frac{q\clrbracket{\theta_{d(t)}|\theta_{d(t+1)}}}{q\clrbracket{\theta_{d(t+1)}|\theta_{d(t)}}}$ is evaluated as follows for $d(t+1)=d(t)+1$ (assuming that $d(t)=k$):
\begin{align}
\frac{q(k|k+1)q(\lambda_{[k]\cdot}|\lambda_{[k+1]\cdot}, idx)q(\pi_{[k]\cdot}|\pi_{[k+1]\cdot}, idx)}{q(k+1|k)q(\lambda_{[k+1]\cdot}|\lambda_{[k]\cdot}, idx)q(\pi_{[k+1]\cdot}|\pi_{[k]\cdot}, idx)}\frac{2\lambda_{[k],idx}}{\mu_1(1-\mu_1)q(\mu_1)}\frac{\pi_{[k]idx}}{q(\mu_2)}
\end{align}
Using the ratio of joint likelihoods in equation \ref{eq:lik_ratio} we can calculate the MH ratio $\alpha$ in equation \ref{eq:alpha}.

\subsection{Gibbs Samplers}
Note that this algorithm will not be sufficient to explore the posterior space. Consider the case when $K=2$. $\lambda_{[1],1}$ will only evaluate to one value, regardless of $\lambda_{12}, \lambda_{22}$ if the current scheme is employed. We employ a Gibbs sampler to efficiently explore each space separately.
\begin{align}
& p(\lambda_{[k],idx}|y,k=1) \propto \lambda_{[k],idx}^{N-1}\exp(-\lambda_{[k],idx}\sum_{i=1}^{N}y_i)\qquad \lambda_{[k],idx}\in[a,b]\\
& p(\lambda_{[k],j}|y,z,k=2) \propto \lambda_{[k],j}^{n_j-1}\exp(-\lambda_{[k],j}\sum_{i\in[j]}y_i)\qquad \lambda_{[k],j}\in[a,b]\\
& p(z_i=j|y,\lambda_{12},\lambda_{22}) \propto \lambda_{[k+1],idx},\lambda_{[k+1],idx+1}\exp(-\lambda_{[k+1],idx},\lambda_{[k+1],idx+1}y_i)\pi\\
& p(\pi) = Dir(\alpha+n_1,\alpha+n_2)
\end{align}

 \section{RJMCMC Algorithm}
 See Algorithm \ref{alg:post}.
\begin{algorithm}
%    \SetAlgoLined
    \KwData{$y_i$, $K$, iterations, $a$, $b$}
    \KwResult{Poseterior values of $\lambda_{[k],\cdot},\pi_{[k],\cdot}$ and state transitions.}
	Initialise $stateChain(1)=1$ and $\lambda_{1}chain(1)\sim p(\lambda|a,b)$\\
    \For{$t =2:iterations$}{
    $\lambda_{old}=\lambda chain\{d(t)\}(last)$\\
    $\pi_{old}=\pi chain\{d(t)\}(last)$\\
    $p_1=p(y,\lambda_{old},\pi_{old},K)$\\
    \eIf{going up}{
    idx = randomly sample from $\{1,2,\cdots,k\}$\\
    $\lambda_{[k+1],/idx}=\lambda_{old,/idx}$\\
%    $\lambda_{[k],idx}=\lambda chain(last)$\\
    $\mu_1,\mu_2\sim  \,U(0,1)$\\
    $\lambda_{[k+1],idx}=\lambda_{old,idx}[\frac{\mu_1}{1-\mu_1}\quad \frac{1-\mu_1}{\mu_1}]$\\
%    $\lambda_{22}=\lambda_{old,idx}$\\
	$\pi_{[k+1],\cdot}=\pi_{old,\cdot}$\\
    $\pi_{[k+1],idx}=\pi_{old,idx}[\mu_2\quad 1-\mu_2]$\\
    $p_2=p(y,\lambda_{[k+1]},\pi_{[k+1]},K)$\\
    $p_{ratio}=p_2/p_1$\\
   	$\lambda_1=\lambda_{old,idx}$\\
   	$\pi_1=\pi_{old,idx}$\\
   	$q_{transition}=q(k+1|k)/q(k|k+1)$
    }{
    idx = randomly sample from $\{1,2,\cdots,k-1\}$\\
    $\lambda_{[k-1],/idx}=\lambda_{old,/idx}$\\
    $\pi_{[k-1],/idx}=\pi_{old,/idx}$\\
%    $\lambda_1=$\\
	$\lambda_{[k-1],idx}=\sqrt{\lambda_{old,idx}\lambda_{old,idx+1}}$\\
%	$\lambda_{[k],idx} = \sqrt{\lambda_{12}\lambda_{22}}$\\
	$\pi_{[k-1],idx}=\pi_{[k],idx}+\pi_{[k],idx+1}$\\
   	$\mu_1 = \frac{\lambda_{[k],idx+1}}{\lambda_{[k-1],idx}+\lambda_{[k],idx+1}}$\\
   	$p_2=p(y,\lambda_{[k-1]},\pi_{[k-1]},K)$\\
   	$p_{ratio}=p_1/p_2$\\
   	$\lambda_1=\lambda_{[k-1],idx}$\\
   	$\pi_1=\pi_{[k-1],idx}$\\
   	$q_{transition}=q(k|k-1)/q(k-1|k)$
    }
	$\alpha_\star=p_{ratio}q_{transition}\frac{2\lambda_{1}\pi_{1}}{\mu_1(1-\mu_1)}$\\
%    \vspace{-1cm}
%    \begin{flalign*}
%    & p(y|\lambda_{12},\lambda_{22},\pi,k=2) = \prod_{i=1}^N(\pi\lambda_{12}\exp(-\lambda_{12}y_i)+(1-\pi)\lambda_{22}\exp(-\lambda_{22}y_i))\\
%    & p(\lambda_{12},\lambda_{22},\pi,k=2) = \frac{1}{C^2}\frac{1}{\lambda_{12}\lambda_{22}}\pi^{\alpha-1}(1-\pi)^{\alpha-1}\frac{1}{K}\\
%    & p(y|\lambda_{[k],idx},k=1) = \lambda_{[k],idx}^N\exp(-\lambda_{[k],idx}\sum_{i=1}^{N}y_i)\\
%    & p(\lambda_{[k],idx},k)=\frac{1}{C}\frac{1}{\lambda_{[k],idx}}\frac{1}{K}\\
%    & \alpha_\star \coloneqq\left( \frac{p(y|\lambda_{12},\lambda_{22},\pi,k=2)p(\lambda_{12},\lambda_{22},\pi,k=2)}{p(y|\lambda_{[k],idx},k=1)p(\lambda_{[k],idx},k)}\right)\left(\frac{2\lambda_{[k],idx}}{\mu_1(1-\mu_1)}\frac{1}{q(\mu_1)q(\mu_2)}\right)
%    \end{flalign*}    
    \If{going down}{
       	$\alpha_\star\coloneqq 1/\alpha_\star$
    }
    $A\coloneqq \min(1,\alpha_\star)$\\
    $u\sim U(0,1)$  \\
    \eIf{$A>u$}{
	    (Accept transition proposal)\\
	    \eIf{going up}{
	    	$\lambda chain\{k+1\}.push(\lambda_{[k+1],\cdot})$\\
	    	$\lambda chain\{k+1\}.push(GibbsSampler(y,a,b))$\\
	    	state chain.push(k+1)
	    }{
	    	$\lambda chain\{k-1\}.push(\lambda_{[k-1],\cdot})$\\
		    $\lambda chain\{k-1\}.push(GibbsSampler(y,a,b))$\\
		    state chain.push(k-1)
	    }	    
	 }{
	    $\lambda chain\{k\}.push(\lambda_{[k],\cdot})$\\
	    $\lambda chain\{k\}.push(GibbsSampler(y,a,b))$\\
		state chain.push(k)
	 }
	}
    \caption{}
    \label{alg:post}
    \end{algorithm}

\clearpage
\section{Results}
For this experiment data is generated as follows:
\begin{align*}
& \pi =  0.4 \\
& \lambda_{12},\lambda_{22} =  [2\quad 6] \\
& z_i \sim \pi^{1(z_i=1)}(1-\pi)^{1(z_i=2)}\qquad i=1,\cdots,N\\
& y_i \sim (p(y_i|\lambda_{12})^{1(z_i=1)}p(y_i|\lambda_{22})^{1(z_i=2)})\qquad i=1,\cdots,N
\end{align*}
where $p(y_i|\lambda_{[k+1],idx},\lambda_{[k+1],idx+1})=\lambda_{[k+1],idx},\lambda_{[k+1],idx+1}\exp(-\lambda_{[k+1],idx},\lambda_{[k+1],idx+1}y_i)$.

The posterior $p(k=1|y)$ is calculated using the simulations and compared against the true posterior. The true posterior is,
\begin{align}
p(k=1|y)=&\frac{p(y|k=1)p(k=1)}{p(y|k=1)p(k=1)+p(y|k=2)p(k=2)}
\end{align}
since the prior $p(k=1)=p(k=2)$, it may be ignored from the above proportion.
\begin{align*}
p(y|k=1)=& \int \prod_{i=1}^{N}p(y_i|\lambda_{[k],idx},k=1)p(\lambda_{[k],idx}) d\lambda_{[k],idx}\\
p(y|k=2)=& \int \prod_{i=1}^{N}\left(\pi p(y_i|\lambda_{12},k=2)+(1-\pi)p(y_i|\lambda_{22},k=2)\right)p(\lambda_{12})p(\lambda_{22})p(\pi) d\lambda_{12}d\lambda_{22}d\pi
\end{align*}
In order to generate the exact posterior, we sample $10^6$ samples from the priors $p(\lambda_{[k],idx})$, $p(\lambda_{12})$, $p(\lambda_{22})$, $p(\pi)$. The posteriors are approximated as:
\begin{align}
p(y|k=1)\approx &\sum_{j=1}^{10^6}\prod_{i=1}^{N}p(y_i|\lambda_{[k],idx}^{(j)},k=1)\\
p(y|k=2)\approx & \sum_{j=1}^{10^6} \prod_{i=1}^{N}\left(\pi^{(j)} p(y_i|\lambda_{12}^{(j)},k=2)+(1-\pi^{(j)})p(y_i|\lambda_{22}^{(j)},k=2)\right)
\end{align}

The above `exact' posterior is compared against the simulated posterior. The relative number of state 1 from stateChain is the posterior probability from the simulated set.

\begin{table}[h]
\centering
\begin{tabular}{llll}
\hline
& \multicolumn{3}{c}{N}\\
\hline
& 50 & 100 & 200 \\
\hline
Exact & 0.63991 & 0.51736 & 0.00048488\\
Simulated & 0.64339 & 0.50834 & 0.00015789\\
\hline
\end{tabular}
\caption{Comparison of simulated and exact posterior of $p(k=1|y)$.}
\end{table}

The posterior $p(\lambda|y,k=1)$ and the posterior $p(\lambda|y,k=2)$ are plotted below in Figure \ref{fig:posteriors} for N=100 case.

\begin{figure}
\centering
\begin{subfigure}{0.3 \textwidth}
\centering
\includegraphics[height=1.5in]{lambda_11}
\caption{}
\end{subfigure}
\begin{subfigure}{0.3 \textwidth}
\centering
\includegraphics[height=1.5in]{lambda_22}
\caption{}
\end{subfigure}
\begin{subfigure}{0.3 \textwidth}
\centering
\includegraphics[height=1.5in]{lambda_22_3d}
\caption{}
\end{subfigure}
\caption{}
\label{fig:posteriors}
\end{figure}

\end{document}