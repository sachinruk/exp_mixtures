\documentclass{article}
\usepackage[]{algorithm2e}
\usepackage{algpseudocode}
\usepackage{amsmath}

\title{Reversible jump MCMC}
\date{}
\begin{document}
\maketitle




Reversible jump MCMC is a Bayesian algorithm to infer the number of components/ clusters from a set of data. For this illustration we shall consider a two component model at most.

\section{Model}
The likelihoods can be represented as:

\begin{align}
p(y_i|\lambda_{11},k=1)=&\lambda_{11}\exp(-\lambda_{11}y_i)\\
p(y_i|\lambda_{12},\lambda_{22},k=2,z_i)=&\prod_j (\lambda_{j2}\exp(-\lambda_{j2}y_i))^{1(z_i=j)}
\end{align}


The priors on the latent variables are:


\begin{align}
p(\lambda_{jk})\propto & \frac{1}{\lambda_{jk}}\qquad \lambda_{jk}\in[a,b]\\
p(z_i=1)=&\pi\\
p(\pi) = & \text{Dir}(\alpha)
p(k=j)= & 1/K
\end{align}


\section{ Jumping dimensions}
We need to consider a Metropolis-Hastings (MH) step to consider going from one component to two components. The MH step in general is as follows:


\begin{align}
\alpha = & \frac{p(y,\theta_2^{t+1})}{p(y,\theta_1^t)}\frac{q(\theta_1^t|\theta_2^{t+1})}{q(\theta_2^{t+1}|\theta_1^{t})}\\
A = & \text{min}\left(1,\alpha\right)
\end{align}


where,


\begin{align}
p(y_i,\theta_2)=& p(y|\lambda_{12},\lambda_{22},\pi)p(\lambda_{12})p(\lambda_{22})p(\pi)\\
=&\pi p(y_i|\lambda_{12})+(1-\pi) p(y_i|\lambda_{22})
\end{align}


\subsection{Jumping from 1 dim to 2}
In this case let the parameters $\theta=\{\cup_j\lambda_{jk},k,\pi\}$ . As we can let the proposal distribution be anything, we let $q(\theta_1\to\theta_2)$ as follows:

\begin{align}
q(\lambda_{j2},\pi,k=2|k=1,\lambda_{11})=q(\lambda_{j2}|k=2,\lambda_{11})q(\pi|k=2)q(k=2|k=1)
\end{align}


We let the **proposal** $q(k=2\vert k=1)=1$. We also have the following dimensional jump:


\begin{align}
\mu_1,\mu_2\sim & U(0,1)\\
\lambda_{12}=&\lambda_{11}\frac{\mu_1}{1-\mu_1}\\
\lambda_{22}=&\lambda_{11}\frac{1-\mu_1}{\mu_1}\\
\pi=&\mu_2
\end{align} 


Thus, in order to find the distribution $q(\lambda_{j2}\vert k=2,\lambda_{11})$ we use the change of variable identity that $q(\lambda_{j2}\vert k=2,\lambda_{11})=q(\mu_1)\vert J\vert$ where, $J$ is the jacobian $\frac{\partial(\lambda_{11},\mu_1)}{\partial(\lambda_{12},\lambda_{22})}$. The Jacobian determinant is found to be $\frac{\mu_1(1-\mu_1)}{2\lambda_{11}}$ while $q(\mu_1)=q(\mu_2)=1$ since they are sampled from standard uniform distributions. Also $q(\mu_2)=q(\pi\vert k=2)$.

Since we need the ratio of proposed states $ \frac{q(\theta_1^t|\theta_2^{t+1})}{q(\theta_2^{t+1}|\theta_1^{t})} $ we are also required to find $ q(\lambda_{11},k=2\vert\lambda_{2j},\pi,k=1) = q(\lambda_{11}\vert\lambda_{2j},k=2) q(k=1 \vert k=2) $. We again take $ q(k=1\vert k=2)=1 $. $q(\lambda_{11}=\sqrt{\lambda_{12}\lambda_{22}})=1$, i.e. support at only point and zero at all other values of $\lambda_{11}$.
 
\subsection{Jumping from 2 to 1}
The MH step is conducted using the reciprocal of $\alpha$ in the equation above.

\section{RJMCMC Algorithm}

\begin{algorithm}
%%randomly generate very first iteration as state 1
%state = 1; 
%idx1 = 1; idx2 = 1; %index keepers
%lambda1 = extremes(1)+(extremes(2)-extremes(1))*rand;
%state_transition(1) = state;
%lambda1_chain(idx1) = lambda1;
%
%idx1 = idx1 +1;
%l=2;
%for i =1:iterations
%    % jump proposals from current state to new state along with new lambdas
%    if state == 1  % q(2 to 1)
%        lambda1 = lambda1_chain(idx1-1);
%        mu = rand(2,1);
%        lambda2 = [lambda1*mu(1)/(1-mu(1)),  lambda1*(1-mu(1))/mu(1)];
%        pi_12 = [mu(2), 1-mu(2)];
%    else % state 2 (q 1 to 2)
%        lambda2 = lambda2_chain(idx2-1,:);
%        lambda1 = sqrt(prod(lambda2));
%        mu(1) = lambda1/(lambda1+lambda2(2));
%    end
%    log_lik = logsumexp(bsxfun(@plus,-(y*lambda2),log(lambda2.*pi_12)), 2);
%    log_joint_lik2 = sum(log_lik)-2*log(normC)-sum(log(lambda2))...
%                     -log(K);
%    log_joint_lik1 = N*log(lambda1)-sum(lambda1*y)-log(normC)-log(lambda1)...
%                    -log(K);
%    logq = log(2)+log(lambda1)-log(mu(1))-log(1-mu(1));
%    alpha_ratio = log_joint_lik2-log_joint_lik1+logq;
%    A = min(0, alpha_ratio);
%    if state == 2
%        A = min(0, -alpha_ratio);
%    end
%
%    if A > log(rand)  % accept move
%        if state == 2
%            state = 1;  % switch states
%            lambda1_chain(idx1) = lambda1;
%            % Gibbs step
%            lambda1 = q_lambda(y, extremes); %gibbs step
%            lambda1_chain(idx1+1) = lambda1; 
%            idx1 =idx1+2;
%        else  % state 1
%            state = 2;  % switch states
%            lambda2_chain(idx2,:) = lambda2;
%            [lambda2, pi_12]=gibbs_sampler2(y, pi_12, lambda2, alpha, extremes);
%            lambda2_chain(idx2+1,:) = lambda2;
%            idx2 = idx2+2;
%        end
%    else  % if rejected proposal, keep old value
%        if state == 2
%            lambda2_chain(idx2,:) = lambda2;
%            [lambda2, pi_12]=gibbs_sampler2(y, pi_12, lambda2, alpha, extremes);
%            lambda2_chain(idx2+1,:) = lambda2;
%            idx2 = idx2+2;
%        else
%            lambda1_chain(idx1) = lambda1;
%            lambda1 = q_lambda(y, extremes); %gibbs step
%            lambda1_chain(idx1+1) = lambda1;
%            idx1=idx1+2;
%        end
%    end
%    state_transition(l:(l+1)) = [state state];
%    l = l + 2;
%end
    \SetAlgoLined
    \KwData{$y_i$}
    \KwResult{Poseterior values of $\lambda_{11},\lambda_{2j}$ and state transitions.}
    %initialization\;
    %state_transition(1) = state;
    %lambda1_chain(idx1) = lambda1;
    %
    %idx1 = idx1 +1;
    %l=2;
    %for i =1:iterations
    %    % jump proposals from current state to new state along with new lambdas
    %    if state == 1  % q(2 to 1)
    %        lambda1 = lambda1_chain(idx1-1);
    %        mu = rand(2,1);
    %        lambda2 = [lambda1*mu(1)/(1-mu(1)),  lambda1*(1-mu(1))/mu(1)];
    %        pi_12 = [mu(2), 1-mu(2)];
    %    else % state 2 (q 1 to 2)
    %        lambda2 = lambda2_chain(idx2-1,:);
    %        lambda1 = sqrt(prod(lambda2));
    %        mu(1) = lambda1/(lambda1+lambda2(2));
    %    end
    %    log_lik = logsumexp(bsxfun(@plus,-(y*lambda2),log(lambda2.*pi_12)), 2);
    %    log_joint_lik2 = sum(log_lik)-2*log(normC)-sum(log(lambda2))...
    %                     -log(K);
    %    log_joint_lik1 = N*log(lambda1)-sum(lambda1*y)-log(normC)-log(lambda1)...
    %                    -log(K);
    \For{U~(0, 1)}{
    \eIf{u < 0.5}{
    $x={ln(\frac{2u}{\theta}}{\theta}$
    }{
    \eIf{u $\geq$ 0.5}{
    $x=-\theta ln(\frac{2-2u}{\theta})$
    }{}
    }}
    \caption{How to generate LaPlace variables from U~(0, 1)}
    \end{algorithm}

\end{document}