\documentclass{article}
\usepackage[]{algorithm2e}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{cleveref}
\usepackage{mathtools}
\title{Reversible jump MCMC}
\date{}
\begin{document}
\maketitle




Reversible jump MCMC is a Bayesian algorithm to infer the number of components/ clusters from a set of data. For this illustration we shall consider a two component model at most.

The difficulty lies in considering the posterior given different number of components.

\section{Model}
The likelihoods can be represented as:

\begin{align}
p(y_i|\lambda_{11},k=1)=&\lambda_{11}\exp(-\lambda_{11}y_i)\\
p(y_i|\lambda_{12},\lambda_{22},k=2,z_i)=&\prod_j (\lambda_{j2}\exp(-\lambda_{j2}y_i))^{1(z_i=j)}
\end{align}


The priors on the latent variables are:
\begin{align}
p(\lambda_{jk})\propto & \frac{1}{\lambda_{jk}}\qquad \lambda_{jk}\in[a,b]\\
p(z_i=1)=&\pi\\
p(\pi) = & \text{Dir}(\alpha)\\
p(k=j)= & 1/K
\end{align}
$z_i$ is an indicator variable which chooses from $\lambda_{12},\lambda_{22}$. The limits on $\lambda_{jk}$ are such that $a=\min(1/y_i)$ and $b=\max(1/y_i)$ for all $j,k$. This is due to the fact that the mean of an exponential distribution is $1/\lambda$. Hence, it is a valid assumption that the mean of $1/y_i$ lies between $a$ and $b$. $\alpha$ is chosen to be $1$ such that it portrays a uniform distribution over $[0,1]$. $K$ is chosen to be $2$ to ensure that each model is equally probable.

\section{Jumping dimensions}
We need to consider a Metropolis-Hastings (MH) step to consider going from one component to two components. The MH step in general is as follows:
\begin{align}
\alpha = & \frac{p(y,\theta_2^{t+1})}{p(y,\theta_1^t)}\frac{q(\theta_1^t|\theta_2^{t+1})}{q(\theta_2^{t+1}|\theta_1^{t})}\\
A = & \text{min}\left(1,\alpha\right)
\end{align}
where,
\begin{align}
p(y_i,\theta_2)=& p(y|\lambda_{12},\lambda_{22},\pi)p(\lambda_{12})p(\lambda_{22})p(\pi)\\
=&\pi p(y_i|\lambda_{12})+(1-\pi) p(y_i|\lambda_{22})
\end{align}
Note that the $z_i$'s have been integrated out leaving $\pi$.

\subsection{Jumping from 1 dim to 2}
In this case let the parameters $\theta_2=\{\lambda_{12},\lambda_{22},k,\pi\}$ and $\theta_1=\{\lambda_{11},k\}$. As we can let the proposal distribution be anything, we let $q(\theta_1\to\theta_2)$ as follows:
\begin{align}
q(\lambda_{j2},\pi,k=2|k=1,\lambda_{11})=q(\lambda_{j2}|k=2,\lambda_{11})q(\pi|k=2)q(k=2|k=1)
\end{align}
We let the \textbf{proposal} $q(k=2\vert k=1)=1$. We also have the following dimensional jump:
\begin{align}
\mu_1,\mu_2\sim & \,U(0,1)\\
\lambda_{12}=&\lambda_{11}\frac{\mu_1}{1-\mu_1}
\label{eq:lambda_12}\\
\lambda_{22}=&\lambda_{11}\frac{1-\mu_1}{\mu_1}
\label{eq:lambda_22}\\
\pi=&\mu_2
\end{align} 

Thus, in order to find the distribution $q(\lambda_{j2}\vert k=2,\lambda_{11})$ we use the change of variable identity that $q(\lambda_{j2}\vert k=2,\lambda_{11})=q(\mu_1)\vert J\vert$ where, $J$ is the Jacobian $\frac{\partial(\lambda_{11},\mu_1)}{\partial(\lambda_{12},\lambda_{22})}$. Using \crefrange{eq:lambda_12}{eq:lambda_22} the inverse Jacobian is found to be:
\begin{align}
J^{-1}=\begin{bmatrix}
\frac{\mu_1}{1-\mu_1} & \lambda_{11}\frac{1}{(1-\mu_1)^2}\\
\frac{1-\mu_1}{\mu_1} & -\lambda_{11}\frac{1}{\mu_1^2}
\end{bmatrix}
\end{align}
Hence the determinant is of the Jacobian determinant is found to be $\frac{\mu_1(1-\mu_1)}{2\lambda_{11}}$ while $q(\mu_1)=q(\mu_2)=1$ since they are sampled from standard uniform distributions. Also $q(\mu_2)=q(\pi\vert k=2)$.

Since we need the ratio of proposed states $ \frac{q(\theta_1^t|\theta_2^{t+1})}{q(\theta_2^{t+1}|\theta_1^{t})} $ we are also required to find $ q(\lambda_{11},k=1\vert\lambda_{2j},\pi,k=2) = q(\lambda_{11}\vert\lambda_{2j},k=1) q(k=1 \vert k=2) $. We again take $ q(k=1\vert k=2)=1 $. $q(\lambda_{11}=\sqrt{\lambda_{12}\lambda_{22}})=1$, i.e. support at only point and zero at all other values of $\lambda_{11}$.
 
\subsection{Jumping from 2 to 1}
The MH step is conducted using the reciprocal of $\alpha$ in the equation above. Apart from the identity that $\lambda_{11}=\sqrt{\lambda_{12}\lambda_{22}}$ we also require the equation, $\mu_1=\frac{\lambda_{12}}{\lambda_{11}+\lambda_{12}}$ which can be derived from equation \ref{eq:lambda_12}. $\mu_2$ which is equivalent to $\pi$ can be taken to be the previous $\pi=\mu_2$ that was generated at state 2.

Note that this algorithm will not be sufficient to explore the posterior space. Consider $\lambda_{11}$ and note that currently it will only evaluate to one value, regardless of $\lambda_{12}, \lambda_{22}$. We employ a Gibbs sampler to efficiently explore each space separately.

 \section{RJMCMC Algorithm}

\begin{algorithm}
%    \SetAlgoLined
    \KwData{$y_i$, iterations, $a$, $b$}
    \KwResult{Poseterior values of $\lambda_{11},\lambda_{2j}$ and state transitions.}
	Initialise $stateChain(1)=1$ and $\lambda_{1}Chain(1)\sim p(\lambda|a,b)$\\
    \For{$i =2:iterations$}{
    \eIf{state is 1}{
    $\lambda_{11}=\lambda_{1}Chain(last)$\\
    $\mu_1,\mu_2\sim  \,U(0,1)$\\
    $\lambda_{12}=\lambda_{11}\frac{\mu_1}{1-\mu_1}$\\
    $\lambda_{22}=\lambda_{11}\frac{1-\mu_1}{\mu_1}$\\
    $\pi=\mu_2$
    }{
	$\lambda_{12},\lambda_{22}=\lambda_{2}Chain(last)$\\
	$\lambda_{11} = \sqrt{\lambda_{12}\lambda_{22}}$\\
   	$\mu_1 = \frac{\lambda_{12}}{\lambda_{11}+\lambda_{12}}$
    }
    \vspace{-1cm}
    \begin{flalign*}
    & p(y|\lambda_{12},\lambda_{22},\pi,k=2) = \prod_{i=1}^N(\pi\lambda_{12}\exp(-\lambda_{12}y_i)+(1-\pi)\lambda_{22}\exp(-\lambda_{22}y_i))\\
    & p(\lambda_{12},\lambda_{22},\pi,k=2) = \frac{1}{C^2}\frac{1}{\lambda_{12}\lambda_{22}}\pi^{\alpha-1}(1-\pi)^{\alpha-1}\frac{1}{K}\\
    & p(y|\lambda_{11},k=1) = \lambda_{11}^N\exp(-\lambda_{11}\sum_{i=1}^{N}y_i)\\
    & p(\lambda_{11},k)=\frac{1}{C}\frac{1}{\lambda_{11}}\frac{1}{K}\\
    & \alpha_\star \coloneqq\left( \frac{p(y|\lambda_{12},\lambda_{22},\pi,k=2)p(\lambda_{12},\lambda_{22},\pi,k=2)}{p(y|\lambda_{11},k=1)p(\lambda_{11},k)}\right)\left(\frac{2\lambda_{11}}{\mu_1(1-\mu_1)}\frac{1}{q(\mu_1)q(\mu_2)}\right)
    \end{flalign*}    
    \If{state is 1}{
       	$\alpha_\star\coloneqq 1/\alpha_\star$
    }
    $A\coloneqq \min(1,\alpha_\star)$\\
    $u\sim U(0,1)$  \\
    \eIf{$A>u$}{
	    (Accept transition proposal)\\
	    \eIf{ state is 2}{
	    	$\lambda_1Chain.push(\lambda_{11})$\\
	    	$\lambda_1Chain.push(GibbsSampler1(y,a,b))$
	    }{
	    	$\lambda_2Chain.push(\lambda_{12},\lambda_{22})$
	    	$\lambda_2Chain.push(GibbsSampler2(y,\pi, \lambda_{12},\lambda_{22},\alpha,a,b))$
	    }
	    Switch states
	 }{\eIf{ state is 2}{
	     $\lambda_2Chain.push(\lambda_{12},\lambda_{22})$
	     	    	$\lambda_2Chain.push(GibbsSampler2(y,\pi, \lambda_{12},\lambda_{22},\alpha,a,b))$
	     }{
	     	$\lambda_1Chain.push(\lambda_{11})$\\
	     	    	$\lambda_1Chain.push(GibbsSampler1(y,a,b))$
	     }
	 }  
	 $stateChain.push([state state])$  
}
%    \caption{How to generate LaPlace variables from U~(0, 1)}
    \end{algorithm}

\end{document}